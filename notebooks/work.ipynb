{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 1. \n",
    "# Import functions.py into notebook from src folder. \n",
    "import sys\n",
    "\n",
    "# Path relative to your notebook.\n",
    "sys.path.append(\"/Users/joel/Desktop/daimil10/capstone_1/DC-Metro-Crime-Data-2007---2017/src\")\n",
    "\n",
    "# Import as usual.\n",
    "import functions\n",
    "\n",
    "# Auto reload.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 2.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read File & Drop First Two Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file in and drop first two rows.\n",
    "# First two rows seem to make like no sense with the rest of this data set.\n",
    "# Plan is to drop them.\n",
    "df = functions.read_file('/Users/joel/Desktop/daimil10/capstone_1/DC-Metro-Crime-Data-2007---2017/data/dc_crime_add_vars.csv')\n",
    "df = df.drop(['Unnamed: 0', 'X'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Dataset 1.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Dataset 2.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Dataset 3.\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Dataset 4.\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Dataset 5.\n",
    "missing_v = df.isnull().sum()\n",
    "missing_v\n",
    "\n",
    "# To viz - make new dataframe of al missing values.\n",
    "data = {\n",
    "    'DISTRICT': [200],\n",
    "    'PSA': [251],\n",
    "    'NEIGHBORHOOD_CLUSTER': [4705],\n",
    "    'BLOCK_GROUP': [1091],\n",
    "    'CENSUS_TRACT': [1091],\n",
    "    'VOTING_PRECINCT': [84],\n",
    "    'START_DATE': [13],\n",
    "    'END_DATE': [11651],\n",
    "    'XBLOCK': [0],\n",
    "    'YBLOCK': [0],\n",
    "    'date': [0],\n",
    "    'year': [0],\n",
    "    'month': [0],\n",
    "    'day': [0],\n",
    "    'hour': [0],\n",
    "    'minute': [0],\n",
    "    'second': [0]\n",
    "}\n",
    "\n",
    "# Show the missing data.\n",
    "df_bar = pd.DataFrame(data)\n",
    "\n",
    "# Figsize.\n",
    "# Use dataframe plot featuer covered in week 4.\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_bar.plot(kind='bar', edgecolor='black')\n",
    "\n",
    "# Labels and title.\n",
    "plt.xlabel('Col Name')\n",
    "plt.ylabel('Missing Values')\n",
    "plt.title('Missing Values in Dataset')\n",
    "\n",
    "# plt.show().\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Dataset 6.\n",
    "# Went from 342867 to 325340 = 17527 removed which is roughly 5% of the dataset. \n",
    "df = functions.drop_rows_with_missing_data(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Dataset 7.\n",
    "# Using the function that formats the nubers and spits out df.describe().\n",
    "functions.describe_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing the Dataset 8\n",
    "# Using the function that spits outdict of col names and dtypes. \n",
    "functions.get_column_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice DF for Only Homicides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use this is need to or can slice the other df accordingly (really dealer choice).\n",
    "# Slice df to only include the offense of homicide.\n",
    "\n",
    "# Convert datetimes.\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html \n",
    "\n",
    "# Make new start and end hour column.\n",
    "# Might need this for the corr matrix? \n",
    "df_2 = df[df['OFFENSE']=='HOMICIDE']\n",
    "df_2['START_DATE'] = pd.to_datetime(df_2['START_DATE'])\n",
    "df_2['END_DATE'] = pd.to_datetime(df_2['END_DATE'])\n",
    "df_2['start_hour'] = df_2['START_DATE'].dt.hour\n",
    "df_2['end_hour'] = df_2['START_DATE'].dt.hour\n",
    "df_2 = df_2.drop('hour', axis=1)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use this is need to or can slice the other df accordingly (same statement as above).\n",
    "\n",
    "# 1100 rows now.\n",
    "hom_df = df_2\n",
    "\n",
    "# Slicing df one last time to get only my desired columns.\n",
    "hom_df = hom_df[['REPORT_DAT',\n",
    "                'SHIFT',\n",
    "                'OFFENSE',\n",
    "                'METHOD',\n",
    "                'BLOCK',\n",
    "                'DISTRICT',\n",
    "                'PSA',\n",
    "                'WARD',\n",
    "                'ANC',\n",
    "                'NEIGHBORHOOD_CLUSTER',\n",
    "                'BLOCK_GROUP',\n",
    "                'VOTING_PRECINCT',\n",
    "                'START_DATE',\n",
    "                'END_DATE',\n",
    "                'XBLOCK',\n",
    "                'YBLOCK',\n",
    "                'date',\n",
    "                'year',\n",
    "                'start_hour',\n",
    "                'end_hour',\n",
    "                'crimetype']]\n",
    "\n",
    "# Show that it worked .. \n",
    "hom_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Have homicides increased over this specified decade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Have homicides increased over this decade?\n",
    "# Line Chart\n",
    "\n",
    "# Slice df or I could use the other one above.. \n",
    "homicides_df = df[df['OFFENSE'] == 'HOMICIDE']\n",
    "\n",
    "# Group the data by year and count the number of homicides per year.\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.size.html\n",
    "homicides_by_year = homicides_df.groupby('year').size()\n",
    "\n",
    "# Make an object out of my PLot class.\n",
    "plotter = functions.Plot()\n",
    "\n",
    "# Use line chart method to make my graph! \n",
    "plotter.line_chart(homicides_by_year, 'Year', 'Homicides', 'Number of Homicides by Year: 2008 - 2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Which voting wards had the highest number of homicides?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Which voting wards had the highest number of homicides?\n",
    "# Bar Chart. \n",
    "\n",
    "# Slice df or I could use the other one above.. \n",
    "homicides_df = df[df['OFFENSE'] == 'HOMICIDE']\n",
    "\n",
    "# Get homicides by ward (need value counts and sort values for descending order).\n",
    "homicides_by_location = homicides_df['WARD'].value_counts().sort_values()\n",
    "\n",
    "# Already have my plotter object from the line chart. \n",
    "# Use bar chart method to make my graph! \n",
    "plotter.bar_chart(homicides_by_location, 'Ward', 'Homicides', 'Number of Homicides by Ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Is there any correlation between the time of the day and the occurrence of homicides?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Is there any correlation between the time of the day and the occurrence of homicides?\n",
    "# Heat Map.\n",
    "# This one gets a little funky.\n",
    "\n",
    "# Extract the hour from the 'REPORT_DAT' column.\n",
    "# Seems to be more stable then the actaul hour column?\n",
    "df['Time_of_Day'] = pd.to_datetime(df['REPORT_DAT']).dt.hour\n",
    "\n",
    "# Create a binary(dummy) column for the category based columns.\n",
    "# https://codereview.stackexchange.com/questions/251690/create-dummy-variables-in-dataframe-using-for-loop-and-apply-lambda\n",
    "df['Homicide'] = df['OFFENSE'].apply(lambda x: 1 if 'HOMICIDE' in x else 0)\n",
    "df['Shift_Evening'] = df['SHIFT'].apply(lambda x: 1 if 'EVENING' in x else 0)\n",
    "df['Shift_Midnight'] = df['SHIFT'].apply(lambda x: 1 if 'MIDNIGHT' in x else 0)\n",
    "\n",
    "# Select the columns of interest for corr df.\n",
    "correlation_df= df[['Time_of_Day', 'Homicide', 'Shift_Evening', 'Shift_Midnight']]\n",
    "corr_matrix = correlation_df.corr()\n",
    "\n",
    "# Already have my plotter object from the line chart. \n",
    "# Use heatmap method to make my graph! \n",
    "plotter.heatmap(corr_matrix, 'Matrix Columns', 'Matrix Column', 'Correlation Heatmap: Homicide & Time of Day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: Are homicides more likely to occur with a gun or a knife? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Are homicides more likely to occur with a gun or a knife? \n",
    "# Stacked Plot.\n",
    "\n",
    "# First name is way to long for graph. \n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html?highlight=replace#pandas.DataFrame.replace\n",
    "df['OFFENSE'] = df['OFFENSE'].replace('ASSAULT W/DANGEROUS WEAPON', 'ASSAULT W/ WEAPON')\n",
    "\n",
    "# Select list of 'violent' offenses to put up next to murder to show contrast. \n",
    "offenses = ['HOMICIDE', 'ROBBERY', 'ASSAULT W/ WEAPON', 'BURGLARY']\n",
    "\n",
    "# Filter the DataFrame using isin cause i keep getting value error when I dont do that?\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html \n",
    "filtered_df = df[df['OFFENSE'].isin(offenses)]\n",
    "\n",
    "# Group the data by offense and method, and count.\n",
    "# .size()is used after grouping the data to count the number of occurrences -\n",
    "# for each group.\n",
    "# .unstack() is applied to a hierarchical index (such as the result of a groupby operation) -\n",
    "# and reshapes the data by \"unstacking\" one level of the index to create columns. \n",
    "# Used the many stacked bar chart examples we have done in the class.\n",
    "grouped_data = filtered_df.groupby(['OFFENSE', 'METHOD']).size().unstack()\n",
    "\n",
    "# Calculate the proportions for each method within each offense.\n",
    "# Wanted to show the precent so just had to times the decimal by 100.\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.div.html was handy even though could of done '/'.\n",
    "grouped_data_proportions = grouped_data.div(grouped_data.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Already have my plotter object from the line chart. \n",
    "# Use stacked method to make my graph!\n",
    "plotter.stacked(grouped_data_proportions, 'Offense', 'Proportion(%)', 'Proportion of Offenses by Method: 2007 - 2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5a: Which voting ward has the highest number of homicides over the last decade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5a: Which voting ward has the highest number of homicides over the last decade?\n",
    "# First of two folium heatmaps. \n",
    "# Filter the DataFrame to include only homicides.\n",
    "homicides_df = df[df['OFFENSE'] == 'HOMICIDE']\n",
    "\n",
    "# Group the data by ward and count the number of homicides in each ward.\n",
    "WARD_homicide_counts = homicides_df['WARD'].value_counts()\n",
    "\n",
    "# Find the ward with the highest amount of homicides.\n",
    "# Used idxmax from previous week's exposure. \n",
    "WARD_with_highest_homicides = WARD_homicide_counts.idxmax()\n",
    "\n",
    "# Filter the DataFrame to include only the data for the ward with the highest amount of homicides.\n",
    "WARD_homicides_df = homicides_df[homicides_df['WARD'] == WARD_with_highest_homicides]\n",
    "\n",
    "# Already have my plotter object from the line chart. \n",
    "# Use folium_heat method to make my graph!\n",
    "plotter.folium_heat(WARD_homicides_df, homicides_df, 'WARD', 'WARD 8', 8, 38.8457, -77.0097)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5b: Which police service area had the highest number of homicides over the last decade?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5b: Which police service area had the highest number of homicides over the last decade?\n",
    "# Second of two folium heatmaps. \n",
    "# Filter the DataFrame to include only homicides.\n",
    "homicides_df = df[df['OFFENSE'] == 'HOMICIDE']\n",
    "\n",
    "# Group the data by PSA and count the number of homicides in each PSA.\n",
    "PSA_homicide_counts = homicides_df['PSA'].value_counts()\n",
    "\n",
    "# Find the PSA with the highest amount of homicides. \n",
    "# Used idxmax from previous week's exposure. \n",
    "PSA_with_highest_homicides = PSA_homicide_counts.idxmax()\n",
    "\n",
    "# Filter the DataFrame to include only the data for the PSA with the highest amount of homicides.\n",
    "PSA_homicides_df = homicides_df[homicides_df['PSA'] == PSA_with_highest_homicides]\n",
    "\n",
    "# Already have my plotter object from the line chart. \n",
    "# Use folium_heat method to make my graph!\n",
    "plotter.folium_heat(PSA_homicides_df, homicides_df, 'PSA', 'PSA 604', 604, 38.8851, -76.9158)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
